{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "based-subcommittee",
   "metadata": {},
   "source": [
    "# Investigación 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-neighborhood",
   "metadata": {},
   "source": [
    "## Aprendizaje automático Interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-literature",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "Si un modelo de aprendizaje automático presenta un buen desempeño ¿porqué no confiar en el modelo e ignorar la razón por la que tomó cierta decisión? La respuesta corta a esta interrogante es porque hay muchos casos en los que una métrica como la exactitud de la predicción no es suficiente para describir las tareas reales.\n",
    "\n",
    "Aunque no hay una definición matemática de la interpretabilidad, varios autores concuerdan en que ésta es el grado en el que un ser humano puede entender la causa de una decisión. Similarmente, se define como el grado en el que un ser humano puede predecir consistentemente el resultado de un modelo. Basado en estas dos definiciones, se puede decir que entre más alta es la interpretabilidad de un modelo de aprendizaje automático, es más fácil para un ser humano el comprender la razón de ciertas decisiones o predicciones hechas por el modelo.\n",
    "\n",
    "Al profundizar en las razones de porqué la interpretabilidad es tan importante, debemos considerar el contexto de nuestro problema. Se reduce a un balance entre:\n",
    " - La predicción que genera el modelo.\n",
    " - Conocer la razón por la que se realizó una predicción\n",
    "\n",
    "Por un lado, el tener un buen puntaje de predicción es deseable, sin embargo en su contraparte, el tener un modelo que predice muy bien implica una complejidad que casi siempre reduce la interpretabilidad de este.\n",
    "\n",
    "Al trabajar en entornos de bajo riesgo, se pueden resolver problemas sin conocer la razón por la que se predice un resultado. Esto se debe a que un error en la predicción no tendrá consecuencias serias. Otra razón por la que puede que no sea necesario explicar o interpretar el comportamiento de un modelo es en los casos en los que el método para resolver el problema ha sido estudiado y evaluado extensivamente.\n",
    "Sin embargo, hay muchos casos en los que el conocer la justificación de una predicción, ayuda a aprender sobre el problema, los datos y la razón por la que un modelo falla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-screen",
   "metadata": {},
   "source": [
    "### Justificación del tema\n",
    "\n",
    "La interpretabilidad se vuelve necesaria al tener un faltante en la formalización de un problema. Esto quiere decir que hay ciertos problemas en los que no es suficiente el obtener una predicción (el qué), si no que también se requiere conocer cómo llegó el modelo a esa predicción (el porqué). Además de esto, las siguientes secciones dirigen la demanda de la interpretabilidad y de las explicaciones:\n",
    "\n",
    "#### Curiosidad y aprendizaje del ser humano\n",
    "\n",
    "El ser humano tiene un modelo mental de su entorno que se actualiza cuando algo inesperado ocurre. Esta actualización se realiza al encontrar una explicación del evento inesperado. Por ejemplo, si una persona se siente enferma de repente y se pregunta: ¿Porqué me siento enfermo? Esta persona aprenderá que se enferma cada vez que come moras. De esta manera actualiza su modelo mental y decide que debe evitar las moras porque lo enferman. Por otro lado, cuando se emplea aprendizaje automático opaco (no se conoce como opera el modelo), los descubrimientos científicos quedan ocultos, ya que el modelo sólo brinda predicciones sin explicaciones. Esta es una razón por la que la interpretabilidad y las explicaciones son cruciales.\n",
    "\n",
    "\n",
    "El deseo de encontrar un significado en el mundo está relacionado con el aprendizaje. El ser humano quiere harmonizar las contradicciones o inconsistencias entre elementos de nuestras estructuras de conocimiento. Por ejemplo: Una persona se puede preguntar ¿Porqué mi perro mordió si nunca lo había hecho antes?. Esto muestra una contradicción entre el comportamiento anterior del perro y el nuevo. El veterinario resuelve esta contradicción en el modelo de conocimiento del dueño: \"El perro estaba bajo estrés y mordió\".\n",
    "\n",
    "De igual manera, entre más se ve afectada la vida de una persona debido a una decisión hecha por un modelo de aprendizaje automático, mayor es la necesidad de explicar el comportamiento del modelo. Por ejemplo: SI un modelo de aprendizaje automático rechaza una solicitud de un préstamo, esto puede ser inesperado para el aplicante. El aplicante va a solicitar una explicación de porqué su expectativa y la realidad son distintas. Y aunque la explicación no tiene que aclarar completamente la situación, debe al menos de cubrir la causa principal.\n",
    "\n",
    "Por otro lado, en muchas disciplinas científicas se tiene un cambio de métodos cualitativos a métodos cuantitativos (sociología, sicología) y hacia el aprendizaje automático. La meta de la ciencia está en la ganancia de conocimiento. La interpretabilidad hace posible la extracción de conocimiento capturado en el modelo.\n",
    "Además, los modelos de aprendizaje automático pueden ser subjetivos según el conjunto de datos de entrenamiento. La interpretabilidad permite detectar estos casos en los modelos de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-victor",
   "metadata": {},
   "source": [
    "Finalmente, si se puede asegurar que el modelo de aprendizaje automático puede explicar decisiones, los siguientes rasgos se pueden revisar más fácilmente:\n",
    "\n",
    "##### Justicia (fairness)\n",
    "Consiste en asegurarse que las predicciones no están parcializadas y no discriminan explícita o implícitamente. Un modelo interpretable puede mostrar la razón por la que tomó una decisión específica. Posteriormente un humano puede juzgar si la decisión está sesgada.\n",
    "\n",
    "##### Privacidad\n",
    "Asegurar que la información sensible en los datos está protegida.\n",
    "\n",
    "##### Confiabilidad o robustez\n",
    "Asegurar que cambios pequeños en la entrada no llevan a grandes cambios en la predicción.\n",
    "\n",
    "##### Causalidad\n",
    "Revisar que sólo se consideran relaciones causales.\n",
    "\n",
    "##### Confianza\n",
    "Es más fácil que los humanos confíen en un sistema que explica sus decisiones al compararlo con una caja negra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-encoding",
   "metadata": {},
   "source": [
    "### Clasificación de métodos\n",
    "\n",
    "Methods for machine learning interpretability can be classified according to various criteria.\n",
    "\n",
    "#### Intrínseco o post-hoc\n",
    "Este criterio distingue si la interpretabilidad se consigue al restringir la complejidad del modelo (intrínseco) o al aplicar métodos que analizan el modelo después de entrenarlo (post-hoc)\n",
    "\n",
    "La interpretabilidad intrínseca se refiere a modelos que se consideran interpretables debido a su estructura simple, tales como árboles de decisión cortos o los modelos lineales. La interpretabilidad Post-hoc se refiere a la aplicación de métodos de interpretación después de entrenar el modelo.\n",
    "\n",
    "#### Estadística de resumen de atributos\n",
    "\n",
    "Muchos métodos de interpretación proveen estadísticas resumen para cada atributo. Algunos retornan un número por atributo (como la importancia de atributo), o un resultado más complejo (como los pares de fortaleza de interacción de atributos).\n",
    "\n",
    "#### Visualización de resumen de atributos\n",
    "\n",
    "La mayoría de las estadísticas pueden ser visualizadas. De hecho, algunos resúmenes de atributos sólo son significativos si son visualizados y una tabla sería una elección errada. Un ejemplo es la dependencia parcial de un atributo. Las gráficas de dependencia parcial son curvas que muestran un atributo y la predicción promedio. La mejor forma para presentar estas curvas es graficar la curva y no solamente los puntos.\n",
    "\n",
    "#### Intra modelo\n",
    "\n",
    "La interpretación de modelos que son intrínsecamente interpretables cae en esta categoría. Un ejemplo de esto son los pesos en los modelos lineales o la estructura aprendida de un árbol (los atributos e intervalos usados para separar) de decisión. La separación entre atributos intra modelo y la estadística resumen de atributos no es clara.\n",
    "\n",
    "#### Puntos de datos\n",
    "Esta categoría incluye todos los métodos que retornan puntos de datos para hacer un modelo interpretable. Para explicar la predicción de una instancia, el método encuentra un punto con datos similares y cambia algunos atributos para los que la predicción cambia de manera relevante (por ejemplo, predecir la clase contraria en una clasificación binaria).\n",
    "\n",
    "#### Específico para un modelo\n",
    "Las herramientas específicas para un modelo están limitadas a clases de modelo específicas. POr ejemplo, la interpretación de los pesos de un modelo de regresión lineal es una interpretación específica del modelo.\n",
    "\n",
    "#### Modelo agnóstico\n",
    "Las herramientas agnósticas del modelo se pueden emplear en cualquier modelo de aprendizaje automático y se aplican después de que el modelo ha sido entrenado (post-hoc). Estos métodos usualmente analizan pares de entrada-salida de atributos. Por definición, estos métodos no pueden tener acceso a datos intra modulares (los pesos, por ejemplo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-sunglasses",
   "metadata": {},
   "source": [
    "### Alcance de la interpretabilidad\n",
    "\n",
    "Un algoritmo entrena un modelo que produce las predicciones. Cada paso pueda evaluarse en términos de transparencia o interpretabilidad.\n",
    "\n",
    "#### Transparencia del algoritmo\n",
    "\n",
    "La transperencia del algoritmo se trata acerca de cómo el algoritmo aprende un modelo basado en los datos y el tipo de relaciones que puede aprender.\n",
    "\n",
    "#### Global, Interpretabilidad de un modelo holístico\n",
    "Se puede describir un modelo como interpretable si se puede comprender el modelo completo a la vez. Para explicar la salida global del modelo, se necesita el modelo entrenado, conocimiento del algoritmo y los datos. Este nivel de interpretabilidad es acerca de la manera en que el modelo toma decisiones, basado en una vista holística de sus atributos y cada uno de los componentes aprendidos, como pesos, otros parámetros y estructuras.\n",
    "Sin embargo, este modelo no es fácil en la práctica.\n",
    "\n",
    "#### Interpretabilidad del modelo global a un nivel modular\n",
    "\n",
    "Como ya se vió, en la práctica no es posible interpretar un modelo de manera global pues sus atributos son muchos para ser manejados manualmente. Sin embargo, se puede analizar el comportamiento de un modelo lineal en función de un peso. También se puede interpretar un árbol de decisiones en función de las decisiones tomadas en cada nivel.\n",
    "\n",
    "No se puede perder de vista que en el caso de los modelos lineales, un único peso está enlazado con los otros pesos. POr lo tanto, la interpretación de un único peso debe asumir que los otross atributos mantienen un valor constante. Claramente esto no es un caso real, pero se puede emplear para facilitar el análisis.\n",
    "\n",
    "#### Interpretabilidad local para un sola predicción\n",
    "\n",
    "Se puede enfocar en una única instancia y examinar la predicción del modelo para esta entrada. Si se observa una única predicción, el comportamiento del modelo es más fácil de comprender. Localmente, la predicción puede depender sólo lineal o monotónicamente en algunos atributos en lugar de tener una dependencia compleja en estos. \n",
    "\n",
    "#### Interpretabilidad local para un grupo de predicciones\n",
    "Las predicciones de un modelo para múltiples instancias se pueden explicar con métodos de interpretación de modelos globales a un nivel modular o con explicaciones de instancias individuales. Los métodos globales se pueden aplicar al tomar el grupo de instancias y tratarlas como si este fuera el conjunto de datos completo y emplear los métodos globales con este sun conjunto. Por otro lado, los métodos individuales se pueden emplear en cada instancia y luego listarlos o acumularlos para el grupo entero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-rescue",
   "metadata": {},
   "source": [
    "### Modelos interpretables\n",
    "\n",
    "#### Regresión lineal\n",
    "\n",
    "##### Interpretación\n",
    "\n",
    "La interpretación de un peso en el modelo de regresión lineal depende en el tipo del atributo.\n",
    "\n",
    "###### Atributo Numérico\n",
    "Aumentar el atributo en una unidad cambia la predicción por su peso. Un ejemplo de este atributo es el tamaño de una casa.\n",
    "\n",
    "###### Atributo binario\n",
    "Toma uno de dos posibles valores en una instancia. Un ejemplo es: \"la casa tiene jardín\". Uno de los valores se denomina la categoría de referencia (0 es un ejemplo). Cambiar a la otra categoría modifica la predicción por el peso del atributo.\n",
    "\n",
    "###### Atributo categórico con múltiples categorías\n",
    "Un atributo con un número fijo de posibles valores. Por ejemplo, el atributo \"tipo de piso\" con categorías \"alfombra\", \"laminado\" o  \"parquet\". Una solución es el conocido \"one-hot encoding\". Este consiste en crear un atributo binario para las categorías posibles. De esta manera se crearían atributos binarios como \"piso de alfombra\", \"piso laminado\" o \"piso parquet\".\n",
    "\n",
    "###### Intercepción\n",
    "\n",
    "Es la constante que se agrega a todas las instancias. La interpretación es: para una instancia con todos sus atributos como 0 y la categoría base como 0, el valor de predicción será la intercepción. Sin embargo, este valor sólo es significativo cuando los atributos han sido estandarizados (media de zero y deviación estándar de 1). En este caso, la intercepción es el valor predicho cuando los atributos están en su valor medio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-internship",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "1) Molnar, Christoph. \"Interpretable machine learning. A Guide for Making Black Box Models Explainable\", 2019. https://christophm.github.io/interpretable-ml-book/.\n",
    "\n",
    "2) Molnar, Christoph,  Giuseppe, Casalicchio and  Bernd, Bischl. \"Interpretable Machine Learning – A Brief History, State-of-the-Art and Challenges\", 2020. https://arxiv.org/pdf/2010.09337.pdf.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
